#!/bin/bash
# Script para executar teste baseline COM RAG + DSPy MANUAL
# FAZ LLM CALLS REAIS com few-shot optimization manual!

echo "================================================================================"
echo "üß™ CrewAI Baseline Test - COM RAG + DSPy MANUAL (LLM CALLS REAIS)"
echo "================================================================================"
echo ""
echo "Este script executa 5 projetos usando DSPy COM RAG e otimiza√ß√£o MANUAL."
echo "‚úÖ FAZ LLM CALLS REAIS (n√£o usa demos auto-compiladas)"
echo "‚úÖ USA few-shot examples manuais para melhorar prompts"
echo ""

# Mudar para diret√≥rio raiz do projeto
cd "$(dirname "$0")/.." || exit 1

# Verificar se API key est√° configurada
echo "Verificando configura√ß√£o..."
API_KEY=$(python -c "from dotenv import load_dotenv; import config; load_dotenv(override=True); print(config.OPENAI_API_KEY[:20] if config.OPENAI_API_KEY else 'None')" 2>/dev/null)
if [ "$API_KEY" = "None" ]; then
    echo "‚ùå OPENAI_API_KEY n√£o configurada!"
    echo ""
    echo "Por favor, configure a API key no arquivo .env:"
    echo "  OPENAI_API_KEY=sk-proj-your-key-here"
    echo ""
    exit 1
fi

if [[ $API_KEY == sk-proj* ]] || [[ $API_KEY == sk-* ]]; then
    echo "‚úÖ API Key configurada: ${API_KEY}..."
else
    echo "‚ö†Ô∏è  Formato de API key inesperado: $API_KEY"
    echo "    Continuando mesmo assim..."
fi
echo ""

# Informar sobre RAG + DSPy Manual
echo "‚úÖ RAG: HABILITADO"
echo "üîß DSPy: OTIMIZA√á√ÉO MANUAL (few-shot examples)"
echo "üí° LLM CALLS: REAIS"
echo ""

# Executar teste
echo "Iniciando teste baseline COM RAG + DSPy Manual..."
echo "Dura√ß√£o estimada: 20-35 minutos (faz LLM calls REAIS!)"
echo "Custo estimado: ~\$3.00-6.00"
echo ""

# Limpar API key cache e executar
unset OPENAI_API_KEY
exec python -c "
import os
import sys

# For√ßar reload do .env
from dotenv import load_dotenv
load_dotenv(override=True)

# Executar teste
sys.path.insert(0, '.')

# Importar e executar teste baseline manual
import time
import json
from pathlib import Path
from datetime import datetime
from crew_dspy_manual import run_software_dev_crew_dspy_manual
from dspy_config import configure_dspy
import config

# Projetos
TEST_PROJECTS = [
    {
        'id': 'project_01',
        'name': 'Todo List CLI',
        'description': 'crie uma aplica√ß√£o CLI para gerenciar lista de tarefas com comandos add, list, done e delete'
    },
    {
        'id': 'project_02',
        'name': 'URL Shortener API',
        'description': 'crie uma API REST para encurtar URLs com endpoints para criar, listar e redirecionar'
    },
    {
        'id': 'project_03',
        'name': 'Weather CLI',
        'description': 'crie uma ferramenta CLI que consulta API de clima e mostra previs√£o formatada'
    },
    {
        'id': 'project_04',
        'name': 'Password Generator',
        'description': 'crie um gerador de senhas seguras CLI com op√ß√µes de tamanho, caracteres especiais e for√ßa'
    },
    {
        'id': 'project_05',
        'name': 'Markdown to HTML Converter',
        'description': 'crie um conversor de Markdown para HTML com suporte a t√≠tulos, listas e links'
    },
]

print()
print('=' * 80)
print('üß™ TESTE BASELINE COM RAG + DSPy MANUAL - BATCH DE 5 PROJETOS')
print('=' * 80)
print()
print('Este teste usa DSPy COM otimiza√ß√£o MANUAL (few-shot).')
print('FAZ LLM CALLS REAIS!')
print()

# Configurar DSPy
print('‚öôÔ∏è  Configurando DSPy...')
configure_dspy()
print()

# Inicializar AgentOps
try:
    import agentops
    if config.AGENTOPS_API_KEY:
        agentops.init(api_key=config.AGENTOPS_API_KEY, tags=['baseline', 'dspy-manual', 'rag'])
        print('‚úÖ AgentOps inicializado')
    else:
        print('‚ö†Ô∏è  AgentOps n√£o configurado')
except:
    print('‚ö†Ô∏è  AgentOps n√£o dispon√≠vel')

print('‚úÖ RAG HABILITADO + DSPy MANUAL (LLM calls reais)')
print()

# Listar projetos
print(f'üìã Projetos a serem executados: {len(TEST_PROJECTS)}')
for i, proj in enumerate(TEST_PROJECTS, 1):
    print(f'   {i}. {proj[\"name\"]} ({proj[\"id\"]})')
print()
print('Iniciando em 3 segundos...')
time.sleep(3)
print()

# Executar batch
all_metrics = []
batch_start = time.time()

for i, project in enumerate(TEST_PROJECTS, 1):
    print()
    print('=' * 80)
    print(f'üìã PROJETO {i}/{len(TEST_PROJECTS)}: {project[\"name\"]}')
    print(f'ID: {project[\"id\"]}')
    print('=' * 80)
    print()

    start = time.time()

    try:
        # Executar pipeline manual (FAZ LLM CALLS REAIS!)
        result = run_software_dev_crew_dspy_manual(
            project_idea=project['description'],
            save_outputs=True
        )

        duration = time.time() - start

        metrics = {
            'project_id': project['id'],
            'project_name': project['name'],
            'status': 'success',
            'duration_seconds': round(duration, 2),
            'timestamp': datetime.now().isoformat(),
            'note': 'Real LLM calls with manual few-shot optimization'
        }

        # Salvar m√©tricas
        metrics_dir = Path('metrics/data/dspy_manual')
        metrics_dir.mkdir(parents=True, exist_ok=True)
        metrics_file = metrics_dir / f'baseline_{project[\"id\"]}.json'
        metrics_file.write_text(json.dumps(metrics, indent=2))

        print()
        print('=' * 80)
        print(f'‚úÖ PROJETO {i} CONCLU√çDO')
        print('=' * 80)
        print(f'‚è±Ô∏è  Dura√ß√£o: {duration:.2f}s')
        print()

        all_metrics.append(metrics)

    except Exception as e:
        duration = time.time() - start
        print(f'‚ùå Error: {e}')
        import traceback
        traceback.print_exc()

        all_metrics.append({
            'project_id': project['id'],
            'project_name': project['name'],
            'status': 'error',
            'duration_seconds': round(duration, 2),
            'error': str(e),
            'timestamp': datetime.now().isoformat(),
        })

    # Pausa entre projetos
    if i < len(TEST_PROJECTS):
        print('\\n‚è∏Ô∏è  Aguardando 5 segundos antes do pr√≥ximo projeto...')
        time.sleep(5)

batch_duration = time.time() - batch_start

# Gerar relat√≥rio
total_duration = sum(m['duration_seconds'] for m in all_metrics)
successful = sum(1 for m in all_metrics if m['status'] == 'success')

print()
print('=' * 80)
print('üìä RELAT√ìRIO BASELINE COM RAG + DSPy MANUAL - RESUMO')
print('=' * 80)
print()
print(f'Total de projetos: {len(all_metrics)}')
print(f'‚úÖ Sucesso: {successful}')
print(f'‚ùå Falhas: {len(all_metrics) - successful}')
print(f'‚è±Ô∏è  Dura√ß√£o total: {total_duration:.2f}s ({total_duration/60:.1f} min)')
print(f'‚è±Ô∏è  Dura√ß√£o m√©dia: {total_duration/len(all_metrics):.2f}s por projeto')
print()

# Salvar relat√≥rio
report_path = Path('metrics/data/dspy_manual/baseline_report.json')
report = {
    'test_type': 'baseline_dspy_manual',
    'test_date': datetime.now().isoformat(),
    'total_projects': len(all_metrics),
    'successful': successful,
    'failed': len(all_metrics) - successful,
    'total_duration_seconds': round(total_duration, 2),
    'avg_duration_per_project_seconds': round(total_duration/len(all_metrics), 2),
    'projects': all_metrics,
}
report_path.write_text(json.dumps(report, indent=2))
print(f'üíæ Relat√≥rio salvo em: {report_path}')

print()
print('=' * 80)
print('üéâ TESTE BASELINE COM RAG + DSPy MANUAL CONCLU√çDO!')
print('=' * 80)
print()
print('Este teste FEZ LLM CALLS REAIS com otimiza√ß√£o manual!')
print()

# Finalizar AgentOps
try:
    import agentops
    agentops.end_session(end_state='Success')
except:
    pass

sys.exit(0)
"
