# ðŸ§ª Guia de Testes Baseline

## O que Ã© o Teste Baseline?

O teste baseline executa **5 projetos em sequÃªncia** para estabelecer mÃ©tricas de referÃªncia do sistema. Essas mÃ©tricas servirÃ£o como base de comparaÃ§Ã£o para futuros testes com:
- RAG otimizado
- AutoPDL/DSPy
- Diferentes configuraÃ§Ãµes

## ðŸ“‹ Projetos de Teste

O baseline inclui 5 projetos variados:

1. **Todo List CLI** - AplicaÃ§Ã£o CLI simples
2. **URL Shortener API** - API REST bÃ¡sica
3. **Weather CLI** - IntegraÃ§Ã£o com API externa
4. **Password Generator** - UtilitÃ¡rio de seguranÃ§a
5. **Markdown to HTML** - Conversor de formatos

Estes projetos foram escolhidos para representar diferentes tipos de tarefas e complexidades.

## ðŸš€ Como Executar

### MÃ©todo 1: Script Bash (Recomendado)

```bash
./run_baseline_test.sh
```

### MÃ©todo 2: Python Direto

```bash
python test_baseline.py
```

## â±ï¸ Tempo de ExecuÃ§Ã£o

- **DuraÃ§Ã£o estimada**: 10-15 minutos
- **DuraÃ§Ã£o por projeto**: ~2-3 minutos
- **Pausa entre projetos**: 5 segundos

## ðŸ“Š MÃ©tricas Coletadas

Para cada projeto, o teste coleta:

### MÃ©tricas de Performance
- â±ï¸ **DuraÃ§Ã£o total** (segundos)
- ðŸš€ **Throughput** (queries/minuto)
- â³ **LatÃªncia por etapa** (retrieval, LLM, verificaÃ§Ã£o)

### MÃ©tricas de Custo
- ðŸ’° **Custo total** (USD)
- ðŸŽ« **Tokens usados** (prompt + completion)
- ðŸ“Š **Custo por token**

### MÃ©tricas de Uso
- ðŸ“ž **LLM calls** (nÃºmero de chamadas)
- ðŸ”§ **Tool calls** (uso de ferramentas)
- ðŸ” **RAG retrievals** (se habilitado)

### MÃ©tricas de Qualidade
- âœ… **Taxa de sucesso** (% de tasks completadas)
- ðŸ“ **Arquivos gerados** (quantidade e tipos)

## ðŸ“ Outputs Gerados

### 1. RelatÃ³rio Consolidado
```
metrics/data/baseline_report.json
```

ContÃ©m:
- EstatÃ­sticas agregadas de todos os projetos
- ComparaÃ§Ã£o entre projetos
- MÃ©dias, totais e distribuiÃ§Ãµes

### 2. MÃ©tricas Individuais
```
metrics/data/baseline_project_01.json
metrics/data/baseline_project_02.json
metrics/data/baseline_project_03.json
metrics/data/baseline_project_04.json
metrics/data/baseline_project_05.json
```

Cada arquivo contÃ©m mÃ©tricas detalhadas de um projeto especÃ­fico.

### 3. Arquivos Gerados pelos Agentes
```
workspace/
â”œâ”€â”€ prd.md
â”œâ”€â”€ architecture.md
â”œâ”€â”€ src/
â”‚   â””â”€â”€ [cÃ³digo gerado]
â”œâ”€â”€ tests.py
â”œâ”€â”€ README.md
â””â”€â”€ user_guide.md
```

## ðŸ“ˆ Exemplo de RelatÃ³rio

```json
{
  "report_type": "baseline",
  "timestamp": "2025-01-12T14:30:00",
  "batch_duration_seconds": 720.5,
  "total_projects": 5,
  "successful_projects": 5,
  "failed_projects": 0,
  "aggregated_stats": {
    "total_cost": 2.5430,
    "avg_cost_per_project": 0.5086,
    "total_tokens": 85340,
    "avg_tokens_per_project": 17068,
    "total_llm_calls": 75,
    "avg_llm_calls_per_project": 15,
    "total_rag_retrievals": 15,
    "avg_duration_per_project": 144.1
  },
  "projects": [...]
}
```

## ðŸŽ¯ Como Usar os Resultados

### 1. AnÃ¡lise Individual
Visualize mÃ©tricas de um projeto especÃ­fico:
```bash
cat metrics/data/baseline_project_01.json | python -m json.tool
```

### 2. AnÃ¡lise Agregada
Visualize o relatÃ³rio consolidado:
```bash
cat metrics/data/baseline_report.json | python -m json.tool
```

### 3. ComparaÃ§Ã£o com Python
```python
import json
from pathlib import Path

# Carregar relatÃ³rio
report = json.loads(Path("metrics/data/baseline_report.json").read_text())

# EstatÃ­sticas agregadas
stats = report['aggregated_stats']
print(f"Custo mÃ©dio: ${stats['avg_cost_per_project']:.4f}")
print(f"Tokens mÃ©dios: {stats['avg_tokens_per_project']:,}")
print(f"DuraÃ§Ã£o mÃ©dia: {stats['avg_duration_per_project']:.1f}s")
```

### 4. ComparaÃ§Ã£o com Futuros Testes
```python
import json

# Carregar baseline e novo teste
baseline = json.loads(Path("metrics/data/baseline_report.json").read_text())
rag_test = json.loads(Path("metrics/data/rag_report.json").read_text())

# Comparar custos
baseline_cost = baseline['aggregated_stats']['avg_cost_per_project']
rag_cost = rag_test['aggregated_stats']['avg_cost_per_project']

improvement = ((baseline_cost - rag_cost) / baseline_cost) * 100
print(f"ReduÃ§Ã£o de custo: {improvement:.1f}%")
```

## ðŸ”¬ Estudos Comparativos

Este baseline Ã© a **Fase 1** do estudo comparativo:

### Fase 1: Baseline (Este teste)
- Sistema atual sem otimizaÃ§Ãµes
- Estabelece mÃ©tricas de referÃªncia

### Fase 2: RAG (PrÃ³ximo)
- Sistema com Retrieval-Augmented Generation
- Compare com baseline

### Fase 3: Otimizado (Futuro)
- Sistema com AutoPDL + DSPy
- Compare com baseline e RAG

Veja [PLANO_ESTUDO_RAG_METRICAS.md](PLANO_ESTUDO_RAG_METRICAS.md) para detalhes.

## ðŸ› ï¸ PersonalizaÃ§Ã£o

### Modificar Projetos de Teste

Edite `test_baseline.py`:

```python
TEST_PROJECTS = [
    {
        "id": "project_01",
        "name": "Seu Projeto",
        "description": "descriÃ§Ã£o do projeto...",
    },
    # Adicione mais projetos...
]
```

### Ajustar MÃ©tricas

Edite as funÃ§Ãµes em `test_baseline.py`:
- `run_single_project()` - Coleta de mÃ©tricas por projeto
- `generate_baseline_report()` - AgregaÃ§Ã£o e relatÃ³rio

### Desabilitar RAG Temporariamente

Para testar baseline puro sem RAG:
```bash
# Renomear knowledge_base
mv knowledge_base knowledge_base.disabled

# Executar teste
./run_baseline_test.sh

# Restaurar
mv knowledge_base.disabled knowledge_base
```

## ðŸ“Š VisualizaÃ§Ã£o de MÃ©tricas

### Terminal
O script imprime resumo automÃ¡tico:
```
================================================================================
ðŸ“Š RELATÃ“RIO BASELINE - RESUMO CONSOLIDADO
================================================================================

Total de projetos: 5
âœ… Sucesso: 5
âŒ Falhas: 0
â±ï¸  DuraÃ§Ã£o total: 720.50s (12.0 min)

--- ESTATÃSTICAS AGREGADAS ---
ðŸ’° Custo total: $2.5430
ðŸ’° Custo mÃ©dio por projeto: $0.5086
ðŸŽ« Tokens totais: 85,340
...
```

### Python Script
Crie script customizado:
```python
# analyze_baseline.py
import json
from pathlib import Path

report = json.loads(Path("metrics/data/baseline_report.json").read_text())

# Suas anÃ¡lises customizadas...
```

### Jupyter Notebook
```python
import json
import pandas as pd
import matplotlib.pyplot as plt

# Carregar dados
report = json.loads(Path("metrics/data/baseline_report.json").read_text())
df = pd.DataFrame(report['projects'])

# Visualizar
df[['project_name', 'duration_seconds', 'metrics.summary.total_cost']].plot()
plt.show()
```

## âš ï¸ ConsideraÃ§Ãµes

### Custos
- Cada projeto usa ~$0.50 em mÃ©dia
- Batch completo: ~$2.50
- Verifique saldo da API antes de executar

### Tempo
- Reserve 15-20 minutos
- NÃ£o interrompa durante execuÃ§Ã£o
- Use `Ctrl+C` se necessÃ¡rio (gera relatÃ³rio parcial)

### Reprodutibilidade
Para resultados consistentes:
- Use mesma temperatura LLM (0.7)
- Execute em horÃ¡rios similares
- Use mesmo modelo (GPT-4)
- Mantenha mesma base de conhecimento

## ðŸ› Troubleshooting

### Erro: "API Key nÃ£o configurada"
```bash
# Configure no .env
echo "OPENAI_API_KEY=sk-proj-your-key" >> .env
```

### Erro: "Rate limit exceeded"
```bash
# Adicione delay maior entre projetos
# Edite test_baseline.py, linha com sleep(5)
time.sleep(30)  # Aumentar para 30s
```

### Workspace cheio
```bash
# Limpar antes de executar
rm -rf workspace/*
./run_baseline_test.sh
```

### Erro em um projeto especÃ­fico
- O teste continua para prÃ³ximos projetos
- Erro Ã© registrado no relatÃ³rio
- Revise logs para debug

## ðŸ“š ReferÃªncias

- [README.md](README.md) - DocumentaÃ§Ã£o principal
- [PLANO_ESTUDO_RAG_METRICAS.md](PLANO_ESTUDO_RAG_METRICAS.md) - Plano de estudo completo
- [RAG_INTEGRATION.md](RAG_INTEGRATION.md) - Como o RAG funciona

---

**Pronto para comeÃ§ar?**

```bash
./run_baseline_test.sh
```

Boa sorte com seus testes! ðŸš€
