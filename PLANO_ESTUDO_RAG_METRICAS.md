# Plano de Estudo: CrewAI com RAG e OtimizaÃ§Ã£o

## ğŸ“‹ VisÃ£o Geral

Este documento descreve um plano completo para implementar e avaliar agentes RAG no sistema CrewAI, com foco em mÃ©tricas de performance e estudo comparativo entre diferentes abordagens de otimizaÃ§Ã£o.

---

## âœ… IDEIA 1: ImplementaÃ§Ã£o de Agente RAG

### **Resposta: SIM, Ã© possÃ­vel e viÃ¡vel**

### Como Aplicar:

#### 1.1 Arquitetura do Agente RAG

```python
# Novo agente: Knowledge Manager (RAG Agent)
- Role: Gerenciador de Base de Conhecimento
- Goal: Fornecer contexto relevante de documentos para outros agentes
- Tools:
  - Document Loader (PDF, MD, TXT, Code)
  - Vector Store (FAISS, ChromaDB, Pinecone)
  - Semantic Search
  - Context Retriever
```

#### 1.2 Componentes NecessÃ¡rios

**A. Document Loader Tool**
```python
@tool("load_documents")
def load_documents_tool(directory: str, file_types: list) -> str:
    """
    Carrega documentos de um diretÃ³rio e cria embeddings.
    - Suporta: .md, .py, .txt, .pdf, .docx
    - Cria embeddings usando OpenAI/HuggingFace
    - Armazena em vector store
    """
```

**B. Semantic Search Tool**
```python
@tool("semantic_search")
def semantic_search_tool(query: str, top_k: int = 5) -> str:
    """
    Busca semÃ¢ntica na base de conhecimento.
    - Converte query em embedding
    - Busca documentos similares
    - Retorna top_k resultados mais relevantes
    """
```

**C. Context Retriever Tool**
```python
@tool("retrieve_context")
def retrieve_context_tool(task_description: str) -> str:
    """
    Recupera contexto relevante para uma tarefa especÃ­fica.
    - Analisa a tarefa
    - Busca documentos relacionados
    - Formata contexto para o LLM
    """
```

#### 1.3 IntegraÃ§Ã£o com Agentes Existentes

```python
# Modificar agents.py para incluir RAG
def create_rag_knowledge_manager() -> Agent:
    return Agent(
        role="Knowledge Manager",
        goal="Retrieve and provide relevant context from knowledge base",
        backstory="""You are a Knowledge Manager specialized in RAG.
        You have access to a comprehensive knowledge base and can:
        - Search for relevant information
        - Retrieve code examples
        - Find best practices and patterns
        - Provide contextual documentation
        """,
        tools=[
            load_documents_tool,
            semantic_search_tool,
            retrieve_context_tool,
            file_reader_tool,
        ],
        verbose=True,
        allow_delegation=False,
    )

# Adicionar RAG Ã s tarefas
def create_prd_task_with_rag(project_idea: str) -> Task:
    return Task(
        description=f"""
        1. Use the Knowledge Manager to search for similar projects
        2. Retrieve best practices for {project_idea}
        3. Create PRD enriched with retrieved context
        """,
        agent=create_product_manager(),
        context=[retrieve_knowledge_task()]  # Nova task de RAG
    )
```

#### 1.4 Base de Conhecimento Sugerida

Crie um diretÃ³rio `knowledge_base/`:
```
knowledge_base/
â”œâ”€â”€ best_practices/
â”‚   â”œâ”€â”€ software_architecture.md
â”‚   â”œâ”€â”€ coding_standards.md
â”‚   â””â”€â”€ testing_strategies.md
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ prd_templates.md
â”‚   â”œâ”€â”€ architecture_templates.md
â”‚   â””â”€â”€ test_plan_templates.md
â”œâ”€â”€ code_examples/
â”‚   â”œâ”€â”€ python_patterns.py
â”‚   â”œâ”€â”€ testing_examples.py
â”‚   â””â”€â”€ api_examples.py
â””â”€â”€ documentation/
    â”œâ”€â”€ project_guidelines.md
    â””â”€â”€ development_process.md
```

#### 1.5 ImplementaÃ§Ã£o de MÃ©tricas BÃ¡sicas

```python
# Adicionar ao RAG agent
import time
from dataclasses import dataclass

@dataclass
class RAGMetrics:
    retrieval_latency: float
    num_documents_retrieved: int
    relevance_score: float
    llm_latency: float
    total_latency: float
    tokens_used: int
    estimated_cost: float

def track_rag_performance(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()

        metrics = RAGMetrics(
            retrieval_latency=end_time - start_time,
            # ... outros campos
        )
        log_metrics(metrics)
        return result
    return wrapper
```

---

## âœ… IDEIA 2: Sistema Completo de MÃ©tricas

### **Resposta: SIM, viÃ¡vel e necessÃ¡rio para estudo cientÃ­fico**

### Como Aplicar:

#### 2.1 MÃ©tricas a Implementar

**A. Latency Tracking**
```python
# metrics_tracker.py
import time
from typing import Dict, List
from datetime import datetime

class MetricsTracker:
    def __init__(self):
        self.metrics = {
            'retrieval_times': [],
            'llm_times': [],
            'verification_times': [],
            'tool_usage': {},
            'agent_success_rate': {},
            'throughput': [],
        }

    def track_stage(self, stage_name: str, duration: float):
        """Registra tempo de cada etapa"""
        self.metrics[f'{stage_name}_times'].append({
            'timestamp': datetime.now(),
            'duration': duration
        })

    def track_retrieval(self, duration: float, docs_retrieved: int):
        """MÃ©tricas de recuperaÃ§Ã£o RAG"""
        self.metrics['retrieval_times'].append({
            'duration': duration,
            'docs_count': docs_retrieved,
            'timestamp': datetime.now()
        })

    def track_llm_call(self, duration: float, tokens: int, cost: float):
        """MÃ©tricas de chamadas LLM"""
        self.metrics['llm_times'].append({
            'duration': duration,
            'tokens': tokens,
            'cost': cost,
            'timestamp': datetime.now()
        })

    def calculate_throughput(self, time_window: int = 60):
        """Queries por minuto"""
        recent_queries = [
            q for q in self.metrics['throughput']
            if (datetime.now() - q['timestamp']).seconds <= time_window
        ]
        return len(recent_queries)
```

**B. Tool Usage Tracking**
```python
class ToolUsageTracker:
    def __init__(self):
        self.tool_calls = {}

    def track_tool_call(self, tool_name: str, success: bool, duration: float):
        if tool_name not in self.tool_calls:
            self.tool_calls[tool_name] = {
                'total_calls': 0,
                'successful_calls': 0,
                'failed_calls': 0,
                'total_duration': 0,
                'avg_duration': 0
            }

        self.tool_calls[tool_name]['total_calls'] += 1
        if success:
            self.tool_calls[tool_name]['successful_calls'] += 1
        else:
            self.tool_calls[tool_name]['failed_calls'] += 1

        self.tool_calls[tool_name]['total_duration'] += duration
        self.tool_calls[tool_name]['avg_duration'] = (
            self.tool_calls[tool_name]['total_duration'] /
            self.tool_calls[tool_name]['total_calls']
        )
```

**C. Agent Success Rate**
```python
class AgentPerformanceTracker:
    def __init__(self):
        self.agent_metrics = {}

    def track_agent_task(self, agent_name: str, task_id: str,
                         success: bool, duration: float, quality_score: float = None):
        if agent_name not in self.agent_metrics:
            self.agent_metrics[agent_name] = {
                'tasks_completed': 0,
                'tasks_failed': 0,
                'success_rate': 0.0,
                'avg_duration': 0.0,
                'quality_scores': []
            }

        metrics = self.agent_metrics[agent_name]
        if success:
            metrics['tasks_completed'] += 1
        else:
            metrics['tasks_failed'] += 1

        total_tasks = metrics['tasks_completed'] + metrics['tasks_failed']
        metrics['success_rate'] = metrics['tasks_completed'] / total_tasks

        if quality_score:
            metrics['quality_scores'].append(quality_score)
```

#### 2.2 IntegraÃ§Ã£o com AgentOps

```python
# enhanced_tracking.py
import agentops
from metrics_tracker import MetricsTracker, ToolUsageTracker, AgentPerformanceTracker

class EnhancedObservability:
    def __init__(self):
        self.metrics_tracker = MetricsTracker()
        self.tool_tracker = ToolUsageTracker()
        self.agent_tracker = AgentPerformanceTracker()
        self.agentops_session = None

    def init_agentops(self, api_key: str):
        """Inicializa AgentOps com mÃ©tricas customizadas"""
        self.agentops_session = agentops.init(
            api_key=api_key,
            default_tags=['rag-study', 'performance-metrics'],
            auto_start_session=True,
            instrument_llm_calls=True,
        )

    def log_custom_metrics(self):
        """Envia mÃ©tricas customizadas para AgentOps"""
        agentops.record({
            'avg_retrieval_latency': self.metrics_tracker.get_avg_retrieval_time(),
            'throughput': self.metrics_tracker.calculate_throughput(),
            'tool_efficiency': self.tool_tracker.get_efficiency_report(),
            'agent_success_rates': self.agent_tracker.get_success_rates()
        })
```

#### 2.3 Dashboard de MÃ©tricas

```python
# metrics_dashboard.py
import json
from pathlib import Path

def generate_metrics_report(metrics_tracker, output_file: str = "metrics_report.json"):
    """Gera relatÃ³rio completo de mÃ©tricas"""
    report = {
        'summary': {
            'total_queries': len(metrics_tracker.metrics['throughput']),
            'avg_latency': calculate_avg_latency(metrics_tracker),
            'total_cost': calculate_total_cost(metrics_tracker),
            'success_rate': calculate_overall_success_rate(metrics_tracker),
        },
        'detailed_metrics': {
            'latency_breakdown': {
                'retrieval': get_avg(metrics_tracker.metrics['retrieval_times']),
                'llm': get_avg(metrics_tracker.metrics['llm_times']),
                'verification': get_avg(metrics_tracker.metrics['verification_times']),
            },
            'tool_usage': metrics_tracker.tool_calls,
            'agent_performance': metrics_tracker.agent_metrics,
            'throughput_over_time': calculate_throughput_timeline(metrics_tracker),
        }
    }

    Path(output_file).write_text(json.dumps(report, indent=2))
    return report
```

---

## âœ… IDEIA 3: Estudo Comparativo (Atual vs RAG vs Otimizado)

### **Resposta: SIM, excelente abordagem para estudo acadÃªmico**

### Como Aplicar:

#### 3.1 Estrutura do Estudo

**Fase 1: Baseline (Sistema Atual)**
```
Objetivo: Estabelecer mÃ©tricas de referÃªncia
DuraÃ§Ã£o: 1 semana
Atividades:
- Executar 50 projetos diferentes
- Coletar mÃ©tricas base
- Documentar limitaÃ§Ãµes
```

**Fase 2: RAG Implementation**
```
Objetivo: Avaliar impacto do RAG
DuraÃ§Ã£o: 2 semanas
Atividades:
- Implementar agente RAG
- Executar mesmos 50 projetos
- Comparar com baseline
- Analisar melhoria de qualidade
```

**Fase 3: Optimization (AutoPDL + DSPy)**
```
Objetivo: Otimizar prompts e fluxo
DuraÃ§Ã£o: 2 semanas
Atividades:
- Aplicar AutoPDL para otimizaÃ§Ã£o de prompts
- Usar DSPy para programaÃ§Ã£o declarativa
- Executar mesmos 50 projetos
- Comparar com Fase 1 e 2
```

#### 3.2 MÃ©tricas de ComparaÃ§Ã£o

```python
# comparison_study.py
from dataclasses import dataclass
from typing import List

@dataclass
class StudyPhase:
    name: str
    avg_latency: float
    avg_cost: float
    success_rate: float
    quality_score: float  # AvaliaÃ§Ã£o manual ou automatizada
    throughput: float

@dataclass
class ComparativeStudy:
    baseline: StudyPhase
    rag_phase: StudyPhase
    optimized_phase: StudyPhase

    def calculate_improvements(self):
        """Calcula melhorias percentuais"""
        return {
            'rag_vs_baseline': {
                'latency_improvement': self.calculate_improvement(
                    self.baseline.avg_latency,
                    self.rag_phase.avg_latency
                ),
                'cost_reduction': self.calculate_improvement(
                    self.baseline.avg_cost,
                    self.rag_phase.avg_cost
                ),
                'quality_improvement': self.calculate_improvement(
                    self.baseline.quality_score,
                    self.rag_phase.quality_score
                ),
            },
            'optimized_vs_baseline': {
                # Similar calculations
            },
            'optimized_vs_rag': {
                # Similar calculations
            }
        }

    def calculate_improvement(self, baseline: float, new: float) -> float:
        """Calcula % de melhoria"""
        return ((new - baseline) / baseline) * 100
```

#### 3.3 Protocolo de Teste

```markdown
## Protocolo de Teste CientÃ­fico

### VariÃ¡veis Controladas:
- Mesmo conjunto de 50 projetos em todas as fases
- Mesma temperatura LLM (0.7)
- Mesmo modelo (GPT-4)
- Mesma base de conhecimento

### VariÃ¡veis Medidas:
1. **Performance**
   - LatÃªncia total (ms)
   - LatÃªncia por etapa (retrieval, LLM, verification)
   - Throughput (projetos/hora)

2. **Custo**
   - Tokens totais
   - Custo em USD
   - Custo por projeto

3. **Qualidade**
   - Taxa de sucesso (builds que funcionam)
   - Cobertura de requisitos (%)
   - Qualidade do cÃ³digo (score 0-10)
   - Completude da documentaÃ§Ã£o (%)

4. **EficiÃªncia**
   - NÃºmero de chamadas LLM
   - NÃºmero de tool calls
   - Taxa de reuso de contexto

### Metodologia:
1. Executar 10 projetos de warm-up (nÃ£o contam)
2. Executar 50 projetos de teste
3. Coletar mÃ©tricas automaticamente
4. AvaliaÃ§Ã£o manual de qualidade (amostra de 10 projetos)
5. AnÃ¡lise estatÃ­stica (t-test, ANOVA)
```

#### 3.4 ImplementaÃ§Ã£o AutoPDL + DSPy

**AutoPDL (Automatic Prompt Design and Learning)**
```python
# autopldl_optimization.py
from dspy import DSPyModule, Signature, ChainOfThought

class OptimizedProductManager(DSPyModule):
    """Product Manager otimizado com DSPy"""

    def __init__(self):
        super().__init__()
        self.generate_prd = ChainOfThought(PRDSignature)

    def forward(self, project_idea: str, knowledge_base_context: str):
        """Gera PRD com contexto otimizado"""
        return self.generate_prd(
            project_idea=project_idea,
            context=knowledge_base_context
        )

class PRDSignature(Signature):
    """Signature para geraÃ§Ã£o de PRD"""
    project_idea: str = dspy.InputField()
    context: str = dspy.InputField(desc="Relevant knowledge from RAG")
    prd: str = dspy.OutputField(desc="Comprehensive PRD document")

# Treinar com exemplos
from dspy.teleprompt import BootstrapFewShot

optimizer = BootstrapFewShot(metric=prd_quality_metric)
optimized_pm = optimizer.compile(
    OptimizedProductManager(),
    trainset=training_examples
)
```

**DSPy Integration**
```python
# dspy_agents.py
import dspy
from dspy import ChainOfThought, Predict

# Configurar LM
lm = dspy.OpenAI(model='gpt-4', max_tokens=4000)
dspy.settings.configure(lm=lm)

class RAGProductManager(dspy.Module):
    def __init__(self):
        super().__init__()
        self.retrieve = dspy.Retrieve(k=5)
        self.generate_prd = ChainOfThought("context, project_idea -> prd")

    def forward(self, project_idea):
        # Retrieve context
        context = self.retrieve(project_idea).passages

        # Generate PRD with context
        prd = self.generate_prd(
            context=context,
            project_idea=project_idea
        )

        return prd
```

#### 3.5 Estrutura de Arquivos do Estudo

```
CrewAI-Project/
â”œâ”€â”€ study/
â”‚   â”œâ”€â”€ phase1_baseline/
â”‚   â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â””â”€â”€ analysis.ipynb
â”‚   â”œâ”€â”€ phase2_rag/
â”‚   â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â””â”€â”€ analysis.ipynb
â”‚   â”œâ”€â”€ phase3_optimized/
â”‚   â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â””â”€â”€ analysis.ipynb
â”‚   â”œâ”€â”€ comparison/
â”‚   â”‚   â”œâ”€â”€ comparative_analysis.ipynb
â”‚   â”‚   â”œâ”€â”€ statistical_tests.py
â”‚   â”‚   â””â”€â”€ final_report.md
â”‚   â””â”€â”€ test_projects/
â”‚       â”œâ”€â”€ project_001.txt
â”‚       â”œâ”€â”€ project_002.txt
â”‚       â””â”€â”€ ... (50 projetos)
â”œâ”€â”€ knowledge_base/  # Base de conhecimento RAG
â”œâ”€â”€ metrics/         # Sistema de mÃ©tricas
â”‚   â”œâ”€â”€ metrics_tracker.py
â”‚   â”œâ”€â”€ tool_usage_tracker.py
â”‚   â””â”€â”€ agent_performance_tracker.py
â”œâ”€â”€ optimization/    # AutoPDL + DSPy
â”‚   â”œâ”€â”€ dspy_agents.py
â”‚   â”œâ”€â”€ autopldl_optimization.py
â”‚   â””â”€â”€ prompt_templates/
â””â”€â”€ rag/            # Sistema RAG
    â”œâ”€â”€ rag_agent.py
    â”œâ”€â”€ vector_store.py
    â””â”€â”€ retriever_tools.py
```

---

## ğŸ“Š Cronograma de ImplementaÃ§Ã£o

### Semana 1-2: Setup e Baseline
- [ ] Criar 50 projetos de teste variados
- [ ] Implementar sistema de mÃ©tricas bÃ¡sico
- [ ] Executar baseline completo
- [ ] Documentar resultados Fase 1

### Semana 3-4: ImplementaÃ§Ã£o RAG
- [ ] Criar estrutura da base de conhecimento
- [ ] Implementar document loader
- [ ] Implementar vector store (FAISS/ChromaDB)
- [ ] Criar agente RAG
- [ ] Integrar RAG com agentes existentes
- [ ] Implementar mÃ©tricas de retrieval

### Semana 5-6: Testes RAG
- [ ] Executar 50 projetos com RAG
- [ ] Coletar mÃ©tricas detalhadas
- [ ] Comparar com baseline
- [ ] AnÃ¡lise preliminar

### Semana 7-8: OtimizaÃ§Ã£o (AutoPDL + DSPy)
- [ ] Instalar e configurar DSPy
- [ ] Converter agentes para DSPy modules
- [ ] Implementar AutoPDL
- [ ] Treinar prompts otimizados
- [ ] Executar 50 projetos otimizados

### Semana 9-10: AnÃ¡lise e RelatÃ³rio
- [ ] AnÃ¡lise estatÃ­stica completa
- [ ] Gerar grÃ¡ficos comparativos
- [ ] Escrever paper/relatÃ³rio cientÃ­fico
- [ ] Preparar apresentaÃ§Ã£o

---

## ğŸ”¬ HipÃ³teses do Estudo

**H1:** RAG melhora a qualidade das entregas em 20-30%
- Medida: Quality score, completude, adequaÃ§Ã£o aos requisitos

**H2:** RAG aumenta latÃªncia em 10-15% mas reduz custo total em 15-25%
- Medida: LatÃªncia total, nÃºmero de LLM calls, tokens usados

**H3:** OtimizaÃ§Ã£o com AutoPDL/DSPy reduz latÃªncia em 25-35% vs baseline
- Medida: LatÃªncia, throughput

**H4:** Sistema otimizado tem taxa de sucesso 40-50% maior que baseline
- Medida: Success rate, builds funcionais

---

## ğŸ“ˆ MÃ©tricas de Sucesso do Estudo

### Quantitativas:
- **Performance**: ReduÃ§Ã£o de 20%+ em latÃªncia mÃ©dia
- **Custo**: ReduÃ§Ã£o de 15%+ em custo por projeto
- **Qualidade**: Aumento de 25%+ em quality score
- **Throughput**: Aumento de 30%+ em projetos/hora

### Qualitativas:
- CÃ³digo mais robusto e bem estruturado
- DocumentaÃ§Ã£o mais completa
- Menor taxa de erros/bugs
- Melhor aderÃªncia aos requisitos

---

## ğŸ› ï¸ Ferramentas e DependÃªncias

```bash
# requirements_study.txt
crewai>=0.28.0
agentops>=0.2.0
langchain>=0.1.0
langchain-openai>=0.0.5
openai>=1.10.0

# RAG
faiss-cpu>=1.7.4  # ou faiss-gpu
chromadb>=0.4.22
sentence-transformers>=2.3.1
pypdf>=4.0.0
python-docx>=1.1.0

# Optimization
dspy-ai>=2.0.0

# Metrics & Analysis
pandas>=2.1.0
numpy>=1.24.0
matplotlib>=3.8.0
seaborn>=0.13.0
scipy>=1.11.0
jupyter>=1.0.0
```

---

## ğŸ“ Template de RelatÃ³rio Final

```markdown
# Estudo Comparativo: CrewAI com RAG e OtimizaÃ§Ã£o

## Abstract
[Resumo dos resultados]

## 1. IntroduÃ§Ã£o
### 1.1 MotivaÃ§Ã£o
### 1.2 Objetivos
### 1.3 ContribuiÃ§Ãµes

## 2. Background
### 2.1 Multi-Agent Systems
### 2.2 RAG (Retrieval-Augmented Generation)
### 2.3 Prompt Optimization (AutoPDL, DSPy)

## 3. Metodologia
### 3.1 ConfiguraÃ§Ã£o Experimental
### 3.2 MÃ©tricas
### 3.3 Protocolo de Teste

## 4. Resultados
### 4.1 Fase 1: Baseline
### 4.2 Fase 2: RAG
### 4.3 Fase 3: Otimizado
### 4.4 AnÃ¡lise Comparativa

## 5. DiscussÃ£o
### 5.1 ValidaÃ§Ã£o de HipÃ³teses
### 5.2 Trade-offs
### 5.3 LimitaÃ§Ãµes

## 6. ConclusÃ£o
### 6.1 Principais Achados
### 6.2 Trabalhos Futuros

## ReferÃªncias
```

---

## ğŸ¯ Resumo Executivo

### âœ… IDEIA 1 - RAG Agent: **VIÃVEL**
- ImplementaÃ§Ã£o estimada: 2 semanas
- Complexidade: MÃ©dia
- Impacto esperado: Alto (qualidade +25%)

### âœ… IDEIA 2 - Sistema de MÃ©tricas: **NECESSÃRIO**
- ImplementaÃ§Ã£o estimada: 1 semana
- Complexidade: Baixa-MÃ©dia
- Impacto esperado: Essencial para estudo cientÃ­fico

### âœ… IDEIA 3 - Estudo Comparativo: **EXCELENTE**
- DuraÃ§Ã£o total: 10 semanas
- Complexidade: Alta
- Impacto esperado: PublicaÃ§Ã£o cientÃ­fica, contribuiÃ§Ã£o acadÃªmica

---

## ğŸš€ PrÃ³ximos Passos Imediatos

1. **Criar base de conhecimento inicial** (1 dia)
   ```bash
   mkdir -p knowledge_base/{best_practices,templates,examples}
   ```

2. **Implementar metrics_tracker.py** (2 dias)
   - MetricsTracker class
   - ToolUsageTracker class
   - AgentPerformanceTracker class

3. **Definir 50 projetos de teste** (1 dia)
   - Variados em complexidade
   - Diferentes domÃ­nios
   - Documentados em test_projects/

4. **Executar baseline** (2-3 dias)
   - Rodar todos os 50 projetos
   - Coletar mÃ©tricas
   - Analisar resultados

5. **Implementar RAG basic** (1 semana)
   - Document loader
   - Vector store
   - Basic retrieval

---

## ğŸ“š ReferÃªncias Ãšteis

- **DSPy**: https://github.com/stanfordnlp/dspy
- **LangChain RAG**: https://python.langchain.com/docs/use_cases/question_answering/
- **FAISS**: https://github.com/facebookresearch/faiss
- **AgentOps**: https://docs.agentops.ai/
- **AutoPDL Paper**: [Link quando disponÃ­vel]

---

**Ãšltima atualizaÃ§Ã£o:** 2025-11-12
**VersÃ£o:** 1.0
**Status:** Plano Aprovado - Pronto para ExecuÃ§Ã£o
